{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_user = 'dsbc_student'\n",
    "postgres_pw = '7*.8G9QH21'\n",
    "postgres_host = '142.93.121.174'\n",
    "postgres_port = '5432'\n",
    "postgres_db = 'useducation'\n",
    "\n",
    "engine = create_engine('postgresql://{}:{}@{}:{}/{}'\n",
    "                       .format(postgres_user, postgres_pw,postgres_host, \n",
    "                        postgres_port, postgres_db))\n",
    "\n",
    "df = pd.read_sql_query('select * from useducation', con=engine)\n",
    "\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Determine all the variable types and find the fraction of the missing values for each variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1492 entries, 0 to 1491\n",
      "Data columns (total 25 columns):\n",
      "PRIMARY_KEY                     1492 non-null object\n",
      "STATE                           1492 non-null object\n",
      "YEAR                            1492 non-null int64\n",
      "ENROLL                          1229 non-null float64\n",
      "TOTAL_REVENUE                   1280 non-null float64\n",
      "FEDERAL_REVENUE                 1280 non-null float64\n",
      "STATE_REVENUE                   1280 non-null float64\n",
      "LOCAL_REVENUE                   1280 non-null float64\n",
      "TOTAL_EXPENDITURE               1280 non-null float64\n",
      "INSTRUCTION_EXPENDITURE         1280 non-null float64\n",
      "SUPPORT_SERVICES_EXPENDITURE    1280 non-null float64\n",
      "OTHER_EXPENDITURE               1229 non-null float64\n",
      "CAPITAL_OUTLAY_EXPENDITURE      1280 non-null float64\n",
      "GRADES_PK_G                     1319 non-null float64\n",
      "GRADES_KG_G                     1360 non-null float64\n",
      "GRADES_4_G                      1361 non-null float64\n",
      "GRADES_8_G                      1361 non-null float64\n",
      "GRADES_12_G                     1361 non-null float64\n",
      "GRADES_1_8_G                    1361 non-null float64\n",
      "GRADES_9_12_G                   1361 non-null float64\n",
      "GRADES_ALL_G                    1319 non-null float64\n",
      "AVG_MATH_4_SCORE                536 non-null float64\n",
      "AVG_MATH_8_SCORE                532 non-null float64\n",
      "AVG_READING_4_SCORE             533 non-null float64\n",
      "AVG_READING_8_SCORE             498 non-null float64\n",
      "dtypes: float64(22), int64(1), object(2)\n",
      "memory usage: 291.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 percent null values in PRIMARY_KEY\n",
      "0.0 percent null values in STATE\n",
      "0.0 percent null values in YEAR\n",
      "17.62734584450402 percent null values in ENROLL\n",
      "14.20911528150134 percent null values in TOTAL_REVENUE\n",
      "14.20911528150134 percent null values in FEDERAL_REVENUE\n",
      "14.20911528150134 percent null values in STATE_REVENUE\n",
      "14.20911528150134 percent null values in LOCAL_REVENUE\n",
      "14.20911528150134 percent null values in TOTAL_EXPENDITURE\n",
      "14.20911528150134 percent null values in INSTRUCTION_EXPENDITURE\n",
      "14.20911528150134 percent null values in SUPPORT_SERVICES_EXPENDITURE\n",
      "17.62734584450402 percent null values in OTHER_EXPENDITURE\n",
      "14.20911528150134 percent null values in CAPITAL_OUTLAY_EXPENDITURE\n",
      "11.595174262734584 percent null values in GRADES_PK_G\n",
      "8.847184986595174 percent null values in GRADES_KG_G\n",
      "8.780160857908847 percent null values in GRADES_4_G\n",
      "8.780160857908847 percent null values in GRADES_8_G\n",
      "8.780160857908847 percent null values in GRADES_12_G\n",
      "8.780160857908847 percent null values in GRADES_1_8_G\n",
      "8.780160857908847 percent null values in GRADES_9_12_G\n",
      "11.595174262734584 percent null values in GRADES_ALL_G\n",
      "64.07506702412869 percent null values in AVG_MATH_4_SCORE\n",
      "64.343163538874 percent null values in AVG_MATH_8_SCORE\n",
      "64.27613941018767 percent null values in AVG_READING_4_SCORE\n",
      "66.62198391420911 percent null values in AVG_READING_8_SCORE\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    null_percent = df[col].isnull().sum()*100/len(df[col])\n",
    "    print('{} percent null values in {}'.format(null_percent, col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Notice that the data has a time dimension (year). For this assignment, forget about time and treat all the observations as if they're from the same year. Choose a strategy to deal with the missing values for each variables. For which variables would filling in the missing values with some value make sense? For which might tossing out the records entirely make sense?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should avoid dropping data, but out of the above columns it makes the most sense to drop the points that have few null values. For example the sets of values that are less than 10% null. \n",
    "\n",
    "Because all the values come from the same year, it would make sense to use the mean as a replacement for missing values. This could be done for the columns that are missing less than 14-20% of the data. \n",
    "\n",
    "For the columns where we are missing close to 65% of the data it doesn't make as much sense to replace the values using the mean because we are missing most of the data. Instead we could limit the scope by dropping these missing values, or instead try to interpolate based on the similarity of other values in the row. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now, take into account the time factor. Replicate your second answer but this time fill in the missing values by using a statistic that is calculated within the year of the observation. For example, if you want to fill a missing value for a variable with the mean of that variable, calculate the mean by using *only* the observations for that specific year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 percent null values in PRIMARY_KEY\n",
      "0.0 percent null values in STATE\n",
      "0.0 percent null values in YEAR\n",
      "7.171581769436997 percent null values in ENROLL\n",
      "3.418230563002681 percent null values in TOTAL_REVENUE\n",
      "3.418230563002681 percent null values in FEDERAL_REVENUE\n",
      "3.418230563002681 percent null values in STATE_REVENUE\n",
      "3.418230563002681 percent null values in LOCAL_REVENUE\n",
      "3.418230563002681 percent null values in TOTAL_EXPENDITURE\n",
      "3.418230563002681 percent null values in INSTRUCTION_EXPENDITURE\n",
      "3.418230563002681 percent null values in SUPPORT_SERVICES_EXPENDITURE\n",
      "7.171581769436997 percent null values in OTHER_EXPENDITURE\n",
      "3.418230563002681 percent null values in CAPITAL_OUTLAY_EXPENDITURE\n",
      "6.836461126005362 percent null values in GRADES_PK_G\n",
      "6.836461126005362 percent null values in GRADES_KG_G\n",
      "6.836461126005362 percent null values in GRADES_4_G\n",
      "6.836461126005362 percent null values in GRADES_8_G\n",
      "6.836461126005362 percent null values in GRADES_12_G\n",
      "6.836461126005362 percent null values in GRADES_1_8_G\n",
      "6.836461126005362 percent null values in GRADES_9_12_G\n",
      "6.836461126005362 percent null values in GRADES_ALL_G\n",
      "57.64075067024129 percent null values in AVG_MATH_4_SCORE\n",
      "57.64075067024129 percent null values in AVG_MATH_8_SCORE\n",
      "57.64075067024129 percent null values in AVG_READING_4_SCORE\n",
      "57.64075067024129 percent null values in AVG_READING_8_SCORE\n"
     ]
    }
   ],
   "source": [
    "df_cpy = df.copy()\n",
    "\n",
    "cols = list(df_cpy.columns)\n",
    "\n",
    "cols = [x for x in cols if x not in ('PRIMARY_KEY', 'STATE', 'YEAR')]\n",
    "\n",
    "unique_yrs = df_cpy['YEAR'].unique()\n",
    "\n",
    "\n",
    "for year in unique_yrs:\n",
    "    for column in cols:       \n",
    "        df_cpy[column].loc[df_cpy['YEAR'] == year] = df_cpy[column].loc[df_cpy['YEAR'] == year].fillna(df_cpy[column].loc[df_cpy['YEAR'] == year].mean())\n",
    "          \n",
    "\n",
    "for col in df_cpy.columns:\n",
    "    null_percent = df_cpy[col].isnull().sum()*100/len(df_cpy[col])\n",
    "    print('{} percent null values in {}'.format(null_percent, col))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. This time, fill in the missing values using interpolation (extrapolation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 percent null values in PRIMARY_KEY\n",
      "0.0 percent null values in STATE\n",
      "0.0 percent null values in YEAR\n",
      "2.4128686327077746 percent null values in ENROLL\n",
      "0.0 percent null values in TOTAL_REVENUE\n",
      "0.0 percent null values in FEDERAL_REVENUE\n",
      "0.0 percent null values in STATE_REVENUE\n",
      "0.0 percent null values in LOCAL_REVENUE\n",
      "0.0 percent null values in TOTAL_EXPENDITURE\n",
      "0.0 percent null values in INSTRUCTION_EXPENDITURE\n",
      "0.0 percent null values in SUPPORT_SERVICES_EXPENDITURE\n",
      "2.4128686327077746 percent null values in OTHER_EXPENDITURE\n",
      "0.0 percent null values in CAPITAL_OUTLAY_EXPENDITURE\n",
      "0.0 percent null values in GRADES_PK_G\n",
      "0.0 percent null values in GRADES_KG_G\n",
      "0.0 percent null values in GRADES_4_G\n",
      "0.0 percent null values in GRADES_8_G\n",
      "0.0 percent null values in GRADES_12_G\n",
      "0.0 percent null values in GRADES_1_8_G\n",
      "0.0 percent null values in GRADES_9_12_G\n",
      "0.0 percent null values in GRADES_ALL_G\n",
      "0.0 percent null values in AVG_MATH_4_SCORE\n",
      "0.0 percent null values in AVG_MATH_8_SCORE\n",
      "0.0 percent null values in AVG_READING_4_SCORE\n",
      "0.06702412868632708 percent null values in AVG_READING_8_SCORE\n"
     ]
    }
   ],
   "source": [
    "df_cpy = df.copy()\n",
    "\n",
    "cols = list(df_cpy.columns)\n",
    "\n",
    "cols = [x for x in cols if x not in ('PRIMARY_KEY', 'STATE', 'YEAR')]\n",
    "\n",
    "unique_yrs = df_cpy['YEAR'].unique()\n",
    "\n",
    "for column in cols:       \n",
    "    df_cpy[column] = df_cpy[column].interpolate()\n",
    "          \n",
    "\n",
    "for col in df_cpy.columns:\n",
    "    null_percent = df_cpy[col].isnull().sum()*100/len(df_cpy[col])\n",
    "    print('{} percent null values in {}'.format(null_percent, col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Compare your results for the 2nd, 3rd, and 4th questions. Do you find any meaningful differences?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolation provided the fewest null values. All columns other than ENROLL, OTHER EXPENDITURE and AVG READING 8 SCORE had a 0 percent null value after interpolation. This was a significant improvement over filling year by year but just because there are fewer null values does not mean that the quality of the data is better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
