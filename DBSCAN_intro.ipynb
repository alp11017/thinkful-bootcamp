{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_user = 'dsbc_student'\n",
    "postgres_pw = '7*.8G9QH21'\n",
    "postgres_host = '142.93.121.174'\n",
    "postgres_port = '5432'\n",
    "postgres_db = 'heartdisease'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://{}:{}@{}:{}/{}'.format(postgres_user, postgres_pw, postgres_host,\n",
    "                                                           postgres_port, postgres_db))\n",
    "\n",
    "df = pd.read_sql_query('SELECT * FROM heartdisease', con=engine)\n",
    "\n",
    "engine.dispose()\n",
    "\n",
    "# Define the features and the outcome\n",
    "X = df.iloc[:, :13]\n",
    "y = df.iloc[:, 13]\n",
    "\n",
    "# Replace missing values (marked by ?) with a 0\n",
    "X = X.replace(to_replace='?', value=0)\n",
    "\n",
    "# Binarize y so that 1 means heart disease diagnosis and 0 means no diagnosis\n",
    "y = np.where(y > 0, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply DBSCAN to the heart disease data by trying different values for eps and min_samples parameters. You'll realize that it's really hard to get a two cluster solution using DBSCAN if not impossible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for eps = 2 and min_samples = 3\n",
      "Adj. Rand Index: 0.02347593686222429\n",
      "Silhouette Score: -0.08928343209927057\n",
      "\n",
      "\n",
      "for eps = 2 and min_samples = 4\n",
      "Adj. Rand Index: 0.018230172604469658\n",
      "Silhouette Score: -0.10841712711520761\n",
      "\n",
      "\n",
      "for eps = 2 and min_samples = 5\n",
      "Adj. Rand Index: 0.014409669379190209\n",
      "Silhouette Score: -0.11852766126558806\n",
      "\n",
      "\n",
      "for eps = 2 and min_samples = 6\n",
      "Adj. Rand Index: 0.014947738155427543\n",
      "Silhouette Score: -0.10960107669691291\n",
      "\n",
      "\n",
      "for eps = 2 and min_samples = 7\n",
      "Adj. Rand Index: 0.013514854268594646\n",
      "Silhouette Score: -0.11140886429253431\n",
      "\n",
      "\n",
      "for eps = 3 and min_samples = 3\n",
      "Adj. Rand Index: 0.020646688946500475\n",
      "Silhouette Score: 0.07069485137365351\n",
      "\n",
      "\n",
      "for eps = 3 and min_samples = 4\n",
      "Adj. Rand Index: 0.018784313764717165\n",
      "Silhouette Score: 0.06913933996869988\n",
      "\n",
      "\n",
      "for eps = 3 and min_samples = 5\n",
      "Adj. Rand Index: 0.01705867403503028\n",
      "Silhouette Score: 0.0035947969725671717\n",
      "\n",
      "\n",
      "for eps = 3 and min_samples = 6\n",
      "Adj. Rand Index: 0.013859023145093265\n",
      "Silhouette Score: 0.045665635045545584\n",
      "\n",
      "\n",
      "for eps = 3 and min_samples = 7\n",
      "Adj. Rand Index: 0.010650198617115567\n",
      "Silhouette Score: 0.1482116657023448\n",
      "\n",
      "\n",
      "for eps = 4 and min_samples = 3\n",
      "Adj. Rand Index: 9.291884165897093e-05\n",
      "Silhouette Score: 0.3369136150427854\n",
      "\n",
      "\n",
      "for eps = 4 and min_samples = 4\n",
      "Adj. Rand Index: 9.291884165897093e-05\n",
      "Silhouette Score: 0.3369136150427854\n",
      "\n",
      "\n",
      "for eps = 4 and min_samples = 5\n",
      "Adj. Rand Index: 9.291884165897093e-05\n",
      "Silhouette Score: 0.3369136150427854\n",
      "\n",
      "\n",
      "for eps = 4 and min_samples = 6\n",
      "Adj. Rand Index: 9.291884165897093e-05\n",
      "Silhouette Score: 0.3369136150427854\n",
      "\n",
      "\n",
      "for eps = 4 and min_samples = 7\n",
      "Adj. Rand Index: 9.291884165897093e-05\n",
      "Silhouette Score: 0.3369136150427854\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#standardizing features\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)\n",
    "\n",
    "eps_val = [2, 3, 4]\n",
    "min_samp = [3, 4, 5, 6, 7]\n",
    "\n",
    "for i in eps_val:\n",
    "    for j in min_samp:\n",
    "        #define DBSCAN cluster\n",
    "        dbscan_cluster = DBSCAN(eps=i, min_samples=j)\n",
    "\n",
    "        clusters = dbscan_cluster.fit_predict(X_std)\n",
    "\n",
    "        #calculating metrics\n",
    "        ari = metrics.adjusted_rand_score(y, clusters)\n",
    "        ss = metrics.silhouette_score(X_std, clusters, metric='euclidean')\n",
    "        \n",
    "        print('for eps = {} and min_samples = {}'.format(i, j))\n",
    "        print('Adj. Rand Index: {}'.format(ari))\n",
    "        print('Silhouette Score: {}'.format(ss))\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply DBSCAN by setting parameters eps=1, min_samples=1, metric=\"euclidean\". Then, increase the value of min_samples. What's the effect of increasing min_samples on the number of clusters DBSCAN identifies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for eps = 1 and min_samples = 1\n",
      "unique clusters: 294\n",
      "\n",
      "\n",
      "for eps = 1 and min_samples = 2\n",
      "unique clusters: 9\n",
      "\n",
      "\n",
      "for eps = 1 and min_samples = 3\n",
      "unique clusters: 2\n",
      "\n",
      "\n",
      "for eps = 1 and min_samples = 4\n",
      "unique clusters: 1\n",
      "\n",
      "\n",
      "for eps = 1 and min_samples = 5\n",
      "unique clusters: 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_samp = [1, 2, 3, 4, 5]\n",
    "\n",
    "for i in min_samp:\n",
    "    #define DBSCAN cluster\n",
    "    dbscan_cluster = DBSCAN(eps=1, min_samples=i, metric='euclidean')\n",
    "    clusters = dbscan_cluster.fit_predict(X_std)\n",
    "\n",
    "    print('for eps = {} and min_samples = {}'.format(1, i))\n",
    "    print('unique clusters: {}'.format(len(np.unique(clusters))))\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By increasing the value of min_samples we can see that the value of clusters shrinks. This is because we are increasing the density requirement. When we have min_samples = 1 then each sample becomes a cluster. As this threshold increases the number of unique clusters drops quickly as we are using more strict definitions of what a 'cluster' is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply DBSCAN by setting parameters eps=1, min_samples=1, metric=\"euclidean\". Then, increase the value of eps. What's the effect of increasing eps on the number of clusters DBSCAN identifies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for eps = 1 and min_samples = 1\n",
      "unique clusters: 294\n",
      "\n",
      "\n",
      "for eps = 2 and min_samples = 1\n",
      "unique clusters: 178\n",
      "\n",
      "\n",
      "for eps = 3 and min_samples = 1\n",
      "unique clusters: 34\n",
      "\n",
      "\n",
      "for eps = 4 and min_samples = 1\n",
      "unique clusters: 3\n",
      "\n",
      "\n",
      "for eps = 5 and min_samples = 1\n",
      "unique clusters: 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eps_val = [1, 2, 3, 4, 5]\n",
    "\n",
    "for i in eps_val:\n",
    "    #define DBSCAN cluster\n",
    "    dbscan_cluster = DBSCAN(eps=i, min_samples=1, metric='euclidean')\n",
    "    clusters = dbscan_cluster.fit_predict(X_std)\n",
    "\n",
    "    print('for eps = {} and min_samples = {}'.format(i, 1))\n",
    "    print('unique clusters: {}'.format(len(np.unique(clusters))))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By increasing the radius around the points of interest (epsilon) we decrease the number of clusters that are identified because we are increasing the area that each cluster covers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
